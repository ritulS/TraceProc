{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace builiding code\n",
    "class Trace:\n",
    "    def __init__(self):\n",
    "        self.calls = []\n",
    "\n",
    "class Node_info:\n",
    "    def __init__(self, num_id, n_type):\n",
    "        self.num_id = num_id\n",
    "        self.n_type = n_type\n",
    "\n",
    "class Call:\n",
    "    def __init__(self, traceid, timestamp, rpcid, um, dm, rpctype, interface, rt):\n",
    "        self.traceid = traceid\n",
    "        self.timestamp = timestamp\n",
    "        self.rpcid = rpcid\n",
    "        self.um = um\n",
    "        self.dm = dm \n",
    "        self.rpctype = rpctype\n",
    "        self.interface = interface\n",
    "        self.rt = rt\n",
    "    def string(self):\n",
    "        return self.traceid + \",\" + str(self.timestamp) + \",\" + self.rpcid + \",\" + self.um + \",\" + self.dm + \",\" +\\\n",
    "            self.rpctype + \",\" + self.interface + \",\" + str(self.rt)\n",
    "\n",
    "def csv_to_df(file: str):\n",
    "    df = pd.read_csv(file,delimiter=',')\n",
    "    return df\n",
    "\n",
    "def extract_traceid_rows(df, tid):\n",
    "    f_df = df[df['traceid'] == tid]\n",
    "    tid_calls = [\n",
    "        Call(\n",
    "            str(row.traceid), \n",
    "            int(row.timestamp), \n",
    "            str(row.rpcid), \n",
    "            str(row.um), \n",
    "            str(row.dm), \n",
    "            str(row.rpctype), \n",
    "            str(row.interface), \n",
    "            int(row.rt)\n",
    "        ) for row in f_df.itertuples(index=False)\n",
    "    ]\n",
    "    return tid_calls\n",
    "\n",
    "def get_call_depth(rpc_id):\n",
    "    if rpc_id == \"0\":\n",
    "        return 1\n",
    "    else:\n",
    "        call_depth = 0\n",
    "        for i in rpc_id:\n",
    "            if i == \".\":\n",
    "                call_depth += 1\n",
    "    return call_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./casper_traces.csv\", 'r') as infile, open(\"./corrected_casper_traces.csv\", 'w') as outfile:\n",
    "    for line in infile:\n",
    "        # Remove trailing comma if present\n",
    "        if line.endswith(',\\n'):\n",
    "            line = line[:-2] + '\\n'\n",
    "        elif line.endswith(','):\n",
    "            line = line[:-1]\n",
    "        outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 666, saw 10\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# types :  ['userDefined', 'db', 'http', 'mq', 'rpc', 'mc']\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Extract to dataframe\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./corrected_casper_traces.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract all tids\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tids_list \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraceid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mcsv_to_df\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcsv_to_df\u001b[39m(file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/TraceProc/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TraceProc/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TraceProc/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/TraceProc/env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 666, saw 10\n"
     ]
    }
   ],
   "source": [
    "# types :  ['userDefined', 'db', 'http', 'mq', 'rpc', 'mc']\n",
    "# Extract to dataframe\n",
    "df = csv_to_df(\"./corrected_casper_traces.csv\")\n",
    "\n",
    "# Extract all tids\n",
    "tids_list = df['traceid'].unique()\n",
    "num_traces = len(tids_list)\n",
    "print(\"Num of tids: \", num_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NIS calc\n",
    "Node Metrics: [type, NIS]\n",
    "'''\n",
    "nis_dict = {}\n",
    "ctr = 0\n",
    "for row in tqdm(df.itertuples(), total=len(df)):\n",
    "    ctr += 1\n",
    "    um_node = row.um\n",
    "    dm_node = row.dm\n",
    "    # Update nis dict for um_node\n",
    "    if um_node in nis_dict:\n",
    "        if row.traceid not in nis_dict[um_node]:\n",
    "            nis_dict[um_node].append(row.traceid)\n",
    "    else:\n",
    "        nis_dict[um_node] = [row.traceid]\n",
    "    # Update nis dict for dm_node\n",
    "    if dm_node in nis_dict:\n",
    "        if row.traceid not in nis_dict[dm_node]:\n",
    "            nis_dict[dm_node].append(row.traceid)\n",
    "    else:\n",
    "        nis_dict[dm_node] = [row.traceid]\n",
    "    # if ctr == 50:\n",
    "    #     break\n",
    "print(len(nis_dict))\n",
    "# replace traceid list with NIS\n",
    "for node in nis_dict:\n",
    "    nis = len(nis_dict[node])/num_traces # TO BE REPLACED\n",
    "    nis_dict[node] = [round(nis,5)]\n",
    "print(nis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trace metrics calc\n",
    "Trace Metrics: [trace_depth, TIS, initial_node, num_sf, num_sl]\n",
    "'''\n",
    "\n",
    "trace_met_dict = {}\n",
    "cd_strat_dict = {} # dict. key: cd, val: [tid ,tis]\n",
    "ctr = 0\n",
    "\n",
    "for i in tqdm(range(len(tids_list))):\n",
    "    ctr += 1\n",
    "    # if i in [0,1,2]:\n",
    "    #     continue\n",
    "    tid_calls = extract_traceid_rows(df, tids_list[i])\n",
    "    initial_node = \"\"\n",
    "    t_edges = [] # list of edges of trace\n",
    "    t_sf_ctr = 0\n",
    "    t_sl_ctr = 0\n",
    "    trace_depth = 0\n",
    "    for call in tid_calls:\n",
    "        t_edges.append([call.um, call.dm])\n",
    "        call_depth = get_call_depth(call.rpcid)\n",
    "        if call_depth > trace_depth: # update trace depth\n",
    "            trace_depth = call_depth\n",
    "        if call.rpcid == \"0\": # get initial node of trace\n",
    "            initial_node = call.um\n",
    "        if call.rpctype == \"db\": # get sf sl node count\n",
    "            t_sf_ctr += 1\n",
    "        else: \n",
    "            t_sl_ctr += 1\n",
    "\n",
    "    # TIS calculation\n",
    "    t_nodes = []\n",
    "    for edge in t_edges:\n",
    "        for node in edge:\n",
    "            if node not in t_nodes:\n",
    "                t_nodes.append(node)\n",
    "    tis = 0\n",
    "    for node in t_nodes:\n",
    "        nis_temp = nis_dict[node][0]\n",
    "        tis += nis_temp \n",
    "    tis = tis/len(t_nodes)\n",
    "\n",
    "    # Collecting data for stratification\n",
    "    if trace_depth not in cd_strat_dict:\n",
    "        cd_strat_dict[trace_depth] = []\n",
    "    cd_strat_dict[trace_depth].append([tids_list[i],tis])\n",
    "\n",
    "    trace_met_dict[tids_list[i]] = [trace_depth, tis, initial_node, t_sf_ctr, t_sl_ctr]\n",
    "\n",
    "    if ctr == 5:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stratification\n",
    "'''\n",
    "t_red = 0 # reduced num of traces (USER DEFINED)\n",
    "cd_strat_arr = [] # [valid call depth, % of tids of resp cd]\n",
    "# ctr is used as the total num of tids\n",
    "\n",
    "# Collecting cd percentage in the original trace\n",
    "for cd in cd_strat_dict:\n",
    "    percent_t_cd = 100 * (len(cd_strat_dict[cd])/ctr)\n",
    "    cd_strat_arr.append[cd, percent_t_cd]\n",
    "    cd_strat_dict[cd] = cd_strat_dict[cd].sort(reverse=True) # sorting tids based on tis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NIS: Priority Sampling\n",
    "'''\n",
    "n = 0 # Reduced num of nodes (USER DEFINED)\n",
    "# Sorted nis_dict based on NIS values\n",
    "sorted_node_items = sorted(nis_dict.items(), key=lambda item: item[1][1], reverse=True)\n",
    "nid_nis_arr_sorted_dict = dict(sorted_node_items)\n",
    "sampled_nodes = list(nid_nis_arr_sorted_dict.keys()[:n]) # List of selected node ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TIS: Priority Sampling\n",
    "'''\n",
    "sampled_tids_inter = [] # T_inter\n",
    "\n",
    "# Priority sampling based on TIS\n",
    "for cd in cd_strat_arr:\n",
    "    num_traces_to_sample_for_cd = t_red * cd[1]\n",
    "    if num_traces_to_sample_for_cd < 1: \n",
    "        continue\n",
    "    cd_traces_list = cd_strat_dict[cd[0]]\n",
    "    cd_priority_sample = cd_traces_list[:num_traces_to_sample_for_cd]\n",
    "    sampled_tids_inter.extend(cd_priority_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Node Pruning\n",
    "Remove all nodes from T_inter that is not in sampled_nodes\n",
    "'''\n",
    "\n",
    "def cycle_func(A, B, pred_flag):\n",
    "    # Cycle through B if it's shorter than A\n",
    "    B_cycle = cycle(B)\n",
    "    # Create tuples by pairing elements from A with elements from the cycled B\n",
    "    if pred_flag == 1: # pred flag is used to check if direction is from pred to succ\n",
    "        result = [(a, next(B_cycle)) for a in A]\n",
    "    else:\n",
    "        result = [(next(B_cycle), a) for a in A]\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_nodes_with_reconnect(G, node):\n",
    "    '''\n",
    "    Remove given node and check if graph breaks. If yes, connect pred nodes\n",
    "    to succ nodes and then remove given node.\n",
    "    returns: Updated graph after removing the given node\n",
    "    '''\n",
    "    G_temp = G.copy()\n",
    "    G_temp.remove_node(node)\n",
    "\n",
    "    if not nx.is_weakly_connected(G_temp): # checking if bridge node\n",
    "        components = list(nx.weakly_connected_components)\n",
    "        if len(components) > 1: # if bridge node\n",
    "            pred_nodes = list(G.predecessors(node))\n",
    "            succ_nodes = list(G.successors(node))\n",
    "            if len(pred_nodes) >= len(succ_nodes):\n",
    "                edges_to_add = cycle_func(pred_nodes, succ_nodes, 1)\n",
    "            else:\n",
    "                edges_to_add = cycle_func(succ_nodes, pred_nodes, 0)\n",
    "            for edge in edges_to_add:\n",
    "                G.add_edge(edge[0], edge[1])\n",
    "    G.remove_node(node)\n",
    "\n",
    "# Sampled_nodes has the selected nodes\n",
    "for tid in sampled_tids_inter:\n",
    "    # build graph\n",
    "    # get list of nodes to remove\n",
    "    # do node pruning\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "def map_elements(A, B):\n",
    "    # Cycle through B if it's shorter than A\n",
    "    B_cycle = cycle(B)\n",
    "    \n",
    "    # Create tuples by pairing elements from A with elements from the cycled B\n",
    "    result = [(a, next(B_cycle)) for a in A]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "A = [1, 2, 3, 4, 5]\n",
    "B = [1, 2, 3]\n",
    "\n",
    "mapped_tuples = map_elements(A, B)\n",
    "print(mapped_tuples[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
